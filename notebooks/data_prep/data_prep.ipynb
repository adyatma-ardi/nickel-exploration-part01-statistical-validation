{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb354baf",
   "metadata": {},
   "source": [
    "## 3.1 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc1583",
   "metadata": {},
   "source": [
    "### 3.1.1 Import & Standardization of Raw CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591739a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper functions\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def load_csv(path):\n",
    "    \"\"\"\n",
    "    Load a CSV file with common options and lowercase columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, sep=\",\") # based on csv separator you use\n",
    "    df.columns = df.columns.str.strip().str.lower()  # normalize column names\n",
    "    return df\n",
    "\n",
    "\n",
    "def enforce_string(df, cols):\n",
    "    \"\"\"\n",
    "    Convert selected columns to string type.\n",
    "    \"\"\"\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "\n",
    "def enforce_float(df, cols):\n",
    "    \"\"\"\n",
    "    Convert selected columns to numeric (float), coercing invalid values to NaN.\n",
    "    \"\"\"\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def enforce_datetime(df, cols):\n",
    "    \"\"\"\n",
    "    Convert selected columns to datetime.\n",
    "    \"\"\"\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def divider(title):\n",
    "    print(f\"\\n{'='*20} {title} {'='*20}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32133fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Collar ====================\n",
      "\n",
      "      hole_id   date   easting_x  northing_y  elevation   eoh  inc_samp\n",
      "0     E07-017  44545  404120.680  114875.100    375.571  16.0        18\n",
      "1     E07-018  44394  404127.929  114849.522    383.153  30.0        34\n",
      "2     E07-019  44545  404157.778  114865.303    371.663  16.0        16\n",
      "3     E07-020  44394  404170.353  114853.791    371.346  10.5        15\n",
      "4     E07-021  44545  404208.368  114873.263    376.459  19.5        21\n",
      "..        ...    ...         ...         ...        ...   ...       ...\n",
      "607  F09-037i  45453  404631.760  114017.846    532.605  14.0        16\n",
      "608  F09-039i  45453  404681.747  114023.668    522.609  10.0        10\n",
      "609  F09-040A  45453  404675.884  113993.707    530.254  15.0        16\n",
      "610  F09-041i  45453  404728.662  114021.932    523.592  10.0        10\n",
      "611  F09-043i  45453  404777.901  114026.667    507.087  10.0        10\n",
      "\n",
      "[612 rows x 7 columns]\n",
      "\n",
      "==================== Assay ====================\n",
      "\n",
      "               sample_id   hole_id  depth_from  depth_to  core_length  cr_%  \\\n",
      "0      B1/E07-017/JAS/01   E07-017         0.0       1.0          1.0   1.0   \n",
      "1      B1/E07-017/JAS/02   E07-017         1.0       1.5          0.5   1.0   \n",
      "2      B1/E07-017/JAS/03   E07-017         1.5       2.0          0.5   1.0   \n",
      "3      B1/E07-017/JAS/04   E07-017         2.0       3.0          1.0   1.0   \n",
      "4      B1/E07-017/JAS/05   E07-017         3.0       3.4          0.4   1.0   \n",
      "...                  ...       ...         ...       ...          ...   ...   \n",
      "9185  B1/F09-043i/JAS/06  F09-043i         5.0       6.0          1.0   1.0   \n",
      "9186  B1/F09-043i/JAS/07  F09-043i         6.0       7.0          1.0   1.0   \n",
      "9187  B1/F09-043i/JAS/08  F09-043i         7.0       8.0          1.0   1.0   \n",
      "9188  B1/F09-043i/JAS/09  F09-043i         8.0       9.0          1.0   1.0   \n",
      "9189  B1/F09-043i/JAS/10  F09-043i         9.0      10.0          1.0   1.0   \n",
      "\n",
      "         ni      fe     mgo    sio2  \n",
      "0     1.140  38.670   2.320  24.150  \n",
      "1     2.080  12.780  20.000  38.100  \n",
      "2     1.920  10.290  23.330  38.800  \n",
      "3     2.020   8.330  25.160  39.490  \n",
      "4     2.130   7.180  26.800  39.610  \n",
      "...     ...     ...     ...     ...  \n",
      "9185  0.287   6.732  31.710  34.689  \n",
      "9186  0.281   6.523  34.241  36.230  \n",
      "9187  0.250   5.790  32.859  35.482  \n",
      "9188  0.248   5.655  33.379  34.603  \n",
      "9189  0.249   5.646  34.173  34.453  \n",
      "\n",
      "[9190 rows x 10 columns]\n",
      "\n",
      "==================== Lithology ====================\n",
      "\n",
      "               sample_id   hole_id  depth_from  depth_to  core_length  cr_%  \\\n",
      "0      B1/E07-017/JAS/01   E07-017         0.0       1.0          1.0   1.0   \n",
      "1      B1/E07-017/JAS/02   E07-017         1.0       1.5          0.5   1.0   \n",
      "2      B1/E07-017/JAS/03   E07-017         1.5       2.0          0.5   1.0   \n",
      "3      B1/E07-017/JAS/04   E07-017         2.0       3.0          1.0   1.0   \n",
      "4      B1/E07-017/JAS/05   E07-017         3.0       3.4          0.4   1.0   \n",
      "...                  ...       ...         ...       ...          ...   ...   \n",
      "9185  B1/F09-043i/JAS/06  F09-043i         5.0       6.0          1.0   1.0   \n",
      "9186  B1/F09-043i/JAS/07  F09-043i         6.0       7.0          1.0   1.0   \n",
      "9187  B1/F09-043i/JAS/08  F09-043i         7.0       8.0          1.0   1.0   \n",
      "9188  B1/F09-043i/JAS/09  F09-043i         8.0       9.0          1.0   1.0   \n",
      "9189  B1/F09-043i/JAS/10  F09-043i         9.0      10.0          1.0   1.0   \n",
      "\n",
      "      lith  \n",
      "0      LIM  \n",
      "1      SAP  \n",
      "2      SAP  \n",
      "3      SAP  \n",
      "4     RSAP  \n",
      "...    ...  \n",
      "9185   SAP  \n",
      "9186   SAP  \n",
      "9187   SAP  \n",
      "9188   SAP  \n",
      "9189   SAP  \n",
      "\n",
      "[9190 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# File paths (modify if needed)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "path_collar    = \"/home/achmad/Projects/data_analyst/EDA_for_NickelExploration/data/raw_collar.csv\"\n",
    "path_assay     = \"/home/achmad/Projects/data_analyst/EDA_for_NickelExploration/data/raw_assay.csv\"\n",
    "path_geology    = \"/home/achmad/Projects/data_analyst/EDA_for_NickelExploration/data/raw_geology.csv\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Import CSV files\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "collar_df    = load_csv(path_collar)\n",
    "assay_df     = load_csv(path_assay)\n",
    "geo_df     = load_csv(path_geology)\n",
    "\n",
    "divider(\"Collar\")\n",
    "print(collar_df)\n",
    "\n",
    "divider(\"Assay\")\n",
    "print(assay_df)\n",
    "\n",
    "divider(\"Lithology\")\n",
    "print(geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de677721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Collar ====================\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 612 entries, 0 to 611\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   hole_id     612 non-null    object        \n",
      " 1   date        612 non-null    datetime64[ns]\n",
      " 2   easting_x   612 non-null    float64       \n",
      " 3   northing_y  612 non-null    float64       \n",
      " 4   elevation   612 non-null    float64       \n",
      " 5   eoh         612 non-null    float64       \n",
      " 6   inc_samp    612 non-null    int64         \n",
      "dtypes: datetime64[ns](1), float64(4), int64(1), object(1)\n",
      "memory usage: 33.6+ KB\n",
      "\n",
      "==================== Assay ====================\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9190 entries, 0 to 9189\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   sample_id    9190 non-null   object \n",
      " 1   hole_id      9190 non-null   object \n",
      " 2   depth_from   9190 non-null   float64\n",
      " 3   depth_to     9190 non-null   float64\n",
      " 4   core_length  9190 non-null   float64\n",
      " 5   cr_%         9190 non-null   float64\n",
      " 6   ni           9190 non-null   float64\n",
      " 7   fe           9190 non-null   float64\n",
      " 8   mgo          9190 non-null   float64\n",
      " 9   sio2         9190 non-null   float64\n",
      "dtypes: float64(8), object(2)\n",
      "memory usage: 718.1+ KB\n",
      "\n",
      "==================== Lithology ====================\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9190 entries, 0 to 9189\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   sample_id    9190 non-null   object \n",
      " 1   hole_id      9190 non-null   object \n",
      " 2   depth_from   9190 non-null   float64\n",
      " 3   depth_to     9190 non-null   float64\n",
      " 4   core_length  9190 non-null   float64\n",
      " 5   cr_%         9190 non-null   float64\n",
      " 6   lith         9190 non-null   object \n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 502.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# # ------------------------------------------------------------\n",
    "# # 2. Standardize column types\n",
    "# # ------------------------------------------------------------\n",
    "\n",
    "# ID fields that must be string (modify based on your schema)\n",
    "string_columns = {\n",
    "    \"collar\":    [\"hole_id\"],\n",
    "    \"assay\":    [\n",
    "        \"sample_id\", \n",
    "        \"hole_id\"\n",
    "    ],\n",
    "    \"geology\":    [\n",
    "        \"sample_id\", \n",
    "        \"hole_id\",\n",
    "        \"length\"\n",
    "    ],    \n",
    "\n",
    "}\n",
    "\n",
    "# # Numeric fields\n",
    "numeric_columns = {\n",
    "    \"collar\":    [\n",
    "        \"easting_x\", \n",
    "        \"northing_y\", \n",
    "        \"elevation\", \n",
    "        \"eoh\", \n",
    "        \"inc_samp\"\n",
    "    ],\n",
    "    \"assay\":    [\n",
    "        \"depth_from\", \n",
    "        \"depth_to\", \n",
    "        \"core_length\", \n",
    "        \"ni\", \n",
    "        \"fe\",\n",
    "        \"mgo\",\n",
    "        \"sio2\"\n",
    "    ],\n",
    "    \"geology\":    [\n",
    "        \"depth_from\", \n",
    "        \"depth_to\", \n",
    "        \"core_length\", \n",
    "    ],     \n",
    "}\n",
    "\n",
    "# # Date fields\n",
    "date_columns = {\n",
    "    \"collar\": [\"date\"],\n",
    "}\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # Apply to each dataset\n",
    "# # ------------------------------------------------------------\n",
    "\n",
    "# collar\n",
    "collar_df = enforce_string(collar_df, string_columns[\"collar\"])\n",
    "collar_df = enforce_float(collar_df, numeric_columns[\"collar\"])\n",
    "collar_df = enforce_datetime(collar_df, date_columns[\"collar\"])\n",
    "\n",
    "# assay\n",
    "assay_df = enforce_string(assay_df, string_columns[\"assay\"])\n",
    "assay_df = enforce_float(assay_df, numeric_columns[\"assay\"])\n",
    "\n",
    "# geology\n",
    "geo_df = enforce_string(geo_df, string_columns[\"geology\"])\n",
    "geo_df = enforce_float(geo_df, numeric_columns[\"geology\"])\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 3. Final output summary\n",
    "# # ------------------------------------------------------------\n",
    "divider(\"Collar\")\n",
    "collar_df.info()\n",
    "\n",
    "divider(\"Assay\")\n",
    "assay_df.info()\n",
    "\n",
    "divider(\"Lithology\")\n",
    "geo_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79fc8bb",
   "metadata": {},
   "source": [
    "### 3.1.2 Data Cleaning & Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d80cb9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper functions\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def divider(title):\n",
    "    print(f\"\\n{'='*20} {title} {'='*20}\\n\")\n",
    "\n",
    "# 1. Duplicate removal\n",
    "def remove_duplicates(df, subset):\n",
    "    before = len(df)\n",
    "\n",
    "    # Identify duplicates BEFORE removal\n",
    "    duplicates = df[df.duplicated(subset=subset, keep=False)].copy()\n",
    "\n",
    "    df_clean = df.drop_duplicates(subset=subset)\n",
    "    removed = before - len(df_clean)\n",
    "\n",
    "    print(f\"Removed {removed} duplicates based on {subset}\")\n",
    "\n",
    "    return df_clean, duplicates\n",
    "\n",
    "\n",
    "# 2. Normalize missing values\n",
    "def normalize_missing(df, essential_cols=None):\n",
    "    error_tokens = [\"#N/A\", \"N/A\", \"NA\", \"\", \"-\", \"--\", \"null\", \"NULL\"]\n",
    "    df = df.replace(error_tokens, np.nan)\n",
    "\n",
    "    if essential_cols:\n",
    "        missing_rows = df[df[essential_cols].isna().any(axis=1)].copy()\n",
    "        df_clean = df.dropna(subset=essential_cols)\n",
    "    else:\n",
    "        missing_rows = pd.DataFrame()\n",
    "        df_clean = df\n",
    "\n",
    "    print(f\"Missing essential rows removed: {len(missing_rows)}\")\n",
    "\n",
    "    return df_clean, missing_rows\n",
    "\n",
    "\n",
    "# 3. Validate intervals\n",
    "def validate_intervals(df, name):\n",
    "    if not {\"depth_from\", \"depth_to\"}.issubset(df.columns):\n",
    "        print(f\"{name}: missing depth interval columns.\")\n",
    "        return df, pd.DataFrame()\n",
    "\n",
    "    df[\"invalid_interval\"] = (df[\"depth_from\"] >= df[\"depth_to\"])\n",
    "\n",
    "    invalid_rows = df[df[\"invalid_interval\"] == True].copy()\n",
    "\n",
    "    print(f\"{name}: Invalid intervals = {len(invalid_rows)}\")\n",
    "\n",
    "    df_clean = df[df[\"invalid_interval\"] == False].copy()\n",
    "\n",
    "    return df_clean, invalid_rows\n",
    "\n",
    "\n",
    "# 4. Detect overlaps per hole\n",
    "def detect_overlaps(df, name):\n",
    "    df = df.sort_values([\"hole_id\", \"depth_from\"])\n",
    "    df[\"overlap\"] = df.groupby(\"hole_id\")[\"depth_from\"].shift(-1) < df[\"depth_to\"]\n",
    "\n",
    "    overlap_rows = df[df[\"overlap\"] == True].copy()\n",
    "    print(f\"{name}: Overlapping intervals = {len(overlap_rows)}\")\n",
    "\n",
    "    df_clean = df.copy()  # you may remove overlaps optionally\n",
    "\n",
    "    return df_clean, overlap_rows\n",
    "\n",
    "\n",
    "# 5. Validate chemical thresholds\n",
    "def validate_chem(df, thresholds, name):\n",
    "    issue_frames = []\n",
    "\n",
    "    for col, (low, high) in thresholds.items():\n",
    "        if col in df.columns:\n",
    "            df[f\"{col}_unrealistic\"] = (df[col] < low) | (df[col] > high)\n",
    "            invalid_rows = df[df[f\"{col}_unrealistic\"] == True].copy()\n",
    "\n",
    "            print(f\"{name}: {col} unrealistic = {len(invalid_rows)}\")\n",
    "\n",
    "            if len(invalid_rows) > 0:\n",
    "                invalid_rows[\"issue_column\"] = col\n",
    "                issue_frames.append(invalid_rows)\n",
    "\n",
    "    # Combine all unrealistic data rows\n",
    "    if len(issue_frames) > 0:\n",
    "        combined_issues = pd.concat(issue_frames, ignore_index=True)\n",
    "    else:\n",
    "        combined_issues = pd.DataFrame()\n",
    "\n",
    "    return df, combined_issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4b598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 duplicates based on ['hole_id']\n",
      "Removed 0 duplicates based on ['sample_id']\n",
      "Removed 0 duplicates based on ['sample_id']\n",
      "Missing essential rows removed: 0\n",
      "Missing essential rows removed: 0\n",
      "Missing essential rows removed: 0\n",
      "Assay: Invalid intervals = 0\n",
      "Lithology: Invalid intervals = 0\n",
      "Assay: Overlapping intervals = 0\n",
      "Lithology: Overlapping intervals = 0\n",
      "Assay: ni unrealistic = 0\n",
      "Assay: fe unrealistic = 0\n",
      "Assay: mgo unrealistic = 200\n",
      "Assay: sio2 unrealistic = 20\n",
      "\n",
      "=== EXPORTING ISSUE LOGS ===\n",
      "No issues for: collar_duplicates\n",
      "No issues for: assay_duplicates\n",
      "No issues for: geo_duplicates\n",
      "No issues for: collar_missing\n",
      "No issues for: assay_missing\n",
      "No issues for: geo_missing\n",
      "No issues for: assay_invalid_interval\n",
      "No issues for: geo_invalid_interval\n",
      "No issues for: assay_overlap\n",
      "No issues for: geo_overlap\n",
      "Saved issue: QAQC_Reports/chemical_unrealistic_20251120_2047.csv  (220 rows)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper functions\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def divider(title):\n",
    "    print(f\"\\n{'='*20} {title} {'='*20}\\n\")\n",
    "\n",
    "# 1. Duplicate removal\n",
    "def remove_duplicates(df, subset):\n",
    "    before = len(df)\n",
    "\n",
    "    # Identify duplicates BEFORE removal\n",
    "    duplicates = df[df.duplicated(subset=subset, keep=False)].copy()\n",
    "\n",
    "    df_clean = df.drop_duplicates(subset=subset)\n",
    "    removed = before - len(df_clean)\n",
    "\n",
    "    print(f\"Removed {removed} duplicates based on {subset}\")\n",
    "\n",
    "    return df_clean, duplicates\n",
    "\n",
    "\n",
    "# 2. Normalize missing values\n",
    "def normalize_missing(df, essential_cols=None):\n",
    "    error_tokens = [\"#N/A\", \"N/A\", \"NA\", \"\", \"-\", \"--\", \"null\", \"NULL\"]\n",
    "    df = df.replace(error_tokens, np.nan)\n",
    "\n",
    "    if essential_cols:\n",
    "        missing_rows = df[df[essential_cols].isna().any(axis=1)].copy()\n",
    "        df_clean = df.dropna(subset=essential_cols)\n",
    "    else:\n",
    "        missing_rows = pd.DataFrame()\n",
    "        df_clean = df\n",
    "\n",
    "    print(f\"Missing essential rows removed: {len(missing_rows)}\")\n",
    "\n",
    "    return df_clean, missing_rows\n",
    "\n",
    "\n",
    "# 3. Validate intervals\n",
    "def validate_intervals(df, name):\n",
    "    if not {\"depth_from\", \"depth_to\"}.issubset(df.columns):\n",
    "        print(f\"{name}: missing depth interval columns.\")\n",
    "        return df, pd.DataFrame()\n",
    "\n",
    "    df[\"invalid_interval\"] = (df[\"depth_from\"] >= df[\"depth_to\"])\n",
    "\n",
    "    invalid_rows = df[df[\"invalid_interval\"] == True].copy()\n",
    "\n",
    "    print(f\"{name}: Invalid intervals = {len(invalid_rows)}\")\n",
    "\n",
    "    df_clean = df[df[\"invalid_interval\"] == False].copy()\n",
    "\n",
    "    return df_clean, invalid_rows\n",
    "\n",
    "\n",
    "# 4. Detect overlaps per hole\n",
    "def detect_overlaps(df, name):\n",
    "    df = df.sort_values([\"hole_id\", \"depth_from\"])\n",
    "    df[\"overlap\"] = df.groupby(\"hole_id\")[\"depth_from\"].shift(-1) < df[\"depth_to\"]\n",
    "\n",
    "    overlap_rows = df[df[\"overlap\"] == True].copy()\n",
    "    print(f\"{name}: Overlapping intervals = {len(overlap_rows)}\")\n",
    "\n",
    "    df_clean = df.copy()  # data are retained; user may correct overlaps manually\n",
    "\n",
    "    return df_clean, overlap_rows\n",
    "\n",
    "\n",
    "# 5. Validate chemical thresholds\n",
    "def validate_chem(df, thresholds, name):\n",
    "    issue_frames = []\n",
    "\n",
    "    for col, (low, high) in thresholds.items():\n",
    "        if col in df.columns:\n",
    "            df[f\"{col}_unrealistic\"] = (df[col] < low) | (df[col] > high)\n",
    "            invalid_rows = df[df[f\"{col}_unrealistic\"] == True].copy()\n",
    "\n",
    "            print(f\"{name}: {col} unrealistic = {len(invalid_rows)}\")\n",
    "\n",
    "            if len(invalid_rows) > 0:\n",
    "                invalid_rows[\"issue_column\"] = col\n",
    "                issue_frames.append(invalid_rows)\n",
    "\n",
    "    # Combine all unrealistic rows\n",
    "    if len(issue_frames) > 0:\n",
    "        combined_issues = pd.concat(issue_frames, ignore_index=True)\n",
    "    else:\n",
    "        combined_issues = pd.DataFrame()\n",
    "\n",
    "    return df, combined_issues\n",
    "    \n",
    "    \n",
    " # ============================================================\n",
    "# CLEANING EXECUTION + COLLECT ISSUE ROWS (WITH TIMESTAMP EXPORT)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create folder for QAQC issue logs\n",
    "os.makedirs(\"log_error\", exist_ok=True)\n",
    "\n",
    "# Timestamp format for filenames\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "all_issues = {}   # dictionary to collect all problematic rows\n",
    "\n",
    "# ---------------- Duplicate Removal ----------------\n",
    "collar_df, collar_dups = remove_duplicates(collar_df, [\"hole_id\"])\n",
    "assay_df,  assay_dups  = remove_duplicates(assay_df, [\"sample_id\"])\n",
    "geo_df,    geo_dups    = remove_duplicates(geo_df, [\"sample_id\"])\n",
    "\n",
    "all_issues[\"collar_duplicates\"] = collar_dups\n",
    "all_issues[\"assay_duplicates\"]  = assay_dups\n",
    "all_issues[\"geo_duplicates\"]    = geo_dups\n",
    "\n",
    "# ---------------- Missing Values -------------------\n",
    "collar_df, collar_missing = normalize_missing(collar_df, [\"hole_id\"])\n",
    "assay_df,  assay_missing  = normalize_missing(assay_df, [\"sample_id\", \"hole_id\"])\n",
    "geo_df,    geo_missing    = normalize_missing(geo_df,   [\"sample_id\", \"hole_id\"])\n",
    "\n",
    "all_issues[\"collar_missing\"] = collar_missing\n",
    "all_issues[\"assay_missing\"]  = assay_missing\n",
    "all_issues[\"geo_missing\"]    = geo_missing\n",
    "\n",
    "# ---------------- Interval Validation --------------\n",
    "assay_df, invalid_assay_int = validate_intervals(assay_df, \"Assay\")\n",
    "geo_df,   invalid_geo_int   = validate_intervals(geo_df,   \"Lithology\")\n",
    "\n",
    "all_issues[\"assay_invalid_interval\"] = invalid_assay_int\n",
    "all_issues[\"geo_invalid_interval\"]   = invalid_geo_int\n",
    "\n",
    "# ---------------- Overlap Detection ----------------\n",
    "assay_df, overlap_assay = detect_overlaps(assay_df, \"Assay\")\n",
    "geo_df,   overlap_geo   = detect_overlaps(geo_df,   \"Lithology\")\n",
    "\n",
    "all_issues[\"assay_overlap\"] = overlap_assay\n",
    "all_issues[\"geo_overlap\"]   = overlap_geo\n",
    "\n",
    "# ---------------- Chemical Validation --------------\n",
    "thresholds = {\n",
    "    \"ni\":  (0, 5),\n",
    "    \"fe\":  (0, 60),\n",
    "    \"mgo\": (0, 40),\n",
    "    \"sio2\": (0, 60),\n",
    "}\n",
    "\n",
    "assay_df, chem_issues = validate_chem(assay_df, thresholds, \"Assay\")\n",
    "all_issues[\"chemical_unrealistic\"] = chem_issues\n",
    "\n",
    "# ============================================================\n",
    "# EXPORT ALL ISSUE LOGS WITH TIMESTAMP\n",
    "# ============================================================\n",
    "print(\"\\n=== EXPORTING ISSUE LOGS ===\")\n",
    "\n",
    "for key, df_issue in all_issues.items():\n",
    "    if df_issue is not None and len(df_issue) > 0:\n",
    "        filename = f\"log_error/{key}_{ts}.csv\"\n",
    "        df_issue.to_csv(filename, index=False)\n",
    "        print(f\"Saved issue: {filename}  ({len(df_issue)} rows)\")\n",
    "    else:\n",
    "        print(f\"No issues for: {key}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nickel_exploration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
