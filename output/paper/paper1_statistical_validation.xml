<?xml version="1.0" encoding="UTF-8"?>
<indexing>
 <paragraph index="6" node_type="writer">Achmad A. Ardi | Applied Python Data Science in Nickel Exploration – Part 1 | Page </paragraph>
 <paragraph index="12" node_type="writer">APPLIED PYTHON DATA SCIENCE IN NICKEL EXPLORATION: PART 1 – STATISTICAL VALIDATION </paragraph>
 <paragraph index="13" node_type="writer">OF NICKEL EXPLORATION ASSAY DATA</paragraph>
 <paragraph index="15" node_type="writer">Achmad Adyatma Ardi, ST</paragraph>
 <paragraph index="16" node_type="writer">Python enthusiast &amp; Geologist</paragraph>
 <paragraph index="17" node_type="writer">adyatmaardi@gmail.com</paragraph>
 <paragraph index="20" node_type="writer">Abstract</paragraph>
 <paragraph index="21" node_type="writer">Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</paragraph>
 <paragraph index="22" node_type="writer">Keywords: …,…,..</paragraph>
 <paragraph index="23" node_type="writer">1. introduction</paragraph>
 <paragraph index="24" node_type="writer">1.1 Background</paragraph>
 <paragraph index="25" node_type="writer">Nickel laterite exploration projects generate extensive assay datasets from multiple drilling campaigns conducted over different time periods and laboratories. These datasets form the foundation for understanding the geochemical characteristics of nickel deposits and for guiding future exploration and resource modeling. However, variations in analytical methods, laboratory calibration, and sampling conditions can introducte bias or inconsistency between datasets collected at different times. To ensure data reliability, a robust statistical validation is required before integrating or comparing datasets. Applying data science methods provides a systematic way to evaluate consistency, identify potential disrepancies, and quantify the degree of agreement between datasets through reproducible statistical workflows.</paragraph>
 <paragraph index="26" node_type="writer">1.2 Objective and Scope of This Paper</paragraph>
 <paragraph index="27" node_type="writer">This paper is Part 1 of the series “Applied Python Data Science in  Nickel Exploration”, which demonstrates a modular data-driven workflow for exploration analysis. The focus of Part 1 is the statistical validation of assay datasets obtained from the different sampling periods and laboratories, collected within the same exploration area. The objective is to determine whether these datasets are statistically consistent and relevant for integrated analysis. Two complementary approaches are conducted:</paragraph>
 <paragraph index="28" node_type="writer">Global Statistical Analysis — to assess the overall consistency between datasets across major geochemical elements (e.g., Ni, Fe, MgO, SiO2)</paragraph>
 <paragraph index="29" node_type="writer">Twin-Hole Analysis — to perform a detailed comparison on spesific drill holes (twin holes) located in close proximity, providing localized validation and understanding of small-scale variation.</paragraph>
 <paragraph index="30" node_type="writer">The results from this paper serve as a foundation for subsequent parts:</paragraph>
 <paragraph index="31" node_type="writer">Part 2 — Multivariate Geochemical Analysis, which will explore inter-element relationships and compositional trends.</paragraph>
 <paragraph index="32" node_type="writer">Part 3 — Spatial &amp; Geostatistical Validation, which will assess spatial continuity and prediction reliability</paragraph>
 <paragraph index="33" node_type="writer">1.3 Structure of the Paper</paragraph>
 <paragraph index="34" node_type="writer">The remainder of this paper is organized as follows:</paragraph>
 <paragraph index="35" node_type="writer">Section 2: Data and Methodology - describes the datasets, preprocessing steps, and statistical methods applied</paragraph>
 <paragraph index="36" node_type="writer">Section 3: Results - presents the outcomes of global statistical comparison and twin-hole validation </paragraph>
 <paragraph index="37" node_type="writer">Section 4: Discussion - interprets the results in the context of data reliability and geochemical consistency</paragraph>
 <paragraph index="38" node_type="writer">Section 5: Conclusion - summarizes key findings and implications for further geochemical and spatial analysis</paragraph>
 <paragraph index="39" node_type="writer">2. Data and Methodology</paragraph>
 <paragraph index="40" node_type="writer">2.1 Dataset Description</paragraph>
 <paragraph index="41" node_type="writer">The datasets used in this study originate from an integrated exploration database comprising collar, geology and assay data. Among these, the assay dataset serve as the primary focus of this analysis, while the collar and geology datasets are maintained as supporting data for subsequent stages of study, including spatial validation and geological modeling.</paragraph>
 <paragraph index="42" node_type="writer">The assay dataset represents nickel laterite chemical analysis collected during two distinct exploration periods. The older assay data, acquired in 2021, consists of 5886 records, while the new dataset, collected during 2023-2024, includes 3305 record, resulting in a total of 9191 samples. The temporal distinction is represented by the boolean field “is_old.”</paragraph>
 <paragraph index="43" node_type="writer">Each assay record includes major elements such as Ni, Fe, MgO and SiO2, along with spatial coordinates (X, Y) and metadata relevant to sampling. A separate boolean field, “is_twin,” identifies paierd or twin-hole samples — 2862 records are classified as twin, and 6329 as non-twin. These paired samples are essential for comparative validation between closely spaced drill holes.</paragraph>
 <paragraph index="44" node_type="writer">All datasets were integrated into a PostgreSQL database through DBeaver, with data ingestion and cleaning performed using Python scripts. This integration enables structured data management and seamless interaction between collar, geology, assay datasets, while providing a reproducible foundation for statistical validation of assay data in this first part of the study.</paragraph>
 <paragraph index="45" node_type="writer">2.2 Data Preprocessing</paragraph>
 <paragraph index="46" node_type="writer">The preprocessing stage involved multiple steps to ensure data quality, consistency, and readiness for statistical analysis. Three primary datasets — collar, assay and geology — were initally provided in CSV format and imported using Python (pandas). During import, spesific parameters for delimiters and character encoding were applied to prevent corruption and ensure proper interpretation of numerical and categorical fields.</paragraph>
 <paragraph index="47" node_type="writer">Some CSV files could not be directly loaded into PostgreSQL due to inconsistency delimiters, special characters, or encoding issues. These files were converted and reformatted using Python scripts to achieve database compatibiity. Invalid entries such as #DIV/0! And #N/A were replaced with null values (pd.NA) to standardize missing data representation and prevent analytical errors.</paragraph>
 <paragraph index="48" node_type="writer">To streamline database integration, the lithology column from the geology dataset was merged into the assay table, reducing the import process from three files to two (collar and assay). This consolidation simplified downstream querying and statistical operations by keeping lithological attributes directly associated with assay results.</paragraph>
 <paragraph index="49" node_type="writer">Additional data enhancement was performed through Excel formulas and VBA macros, including the creation of two Boolean attributes:</paragraph>
 <paragraph index="50" node_type="writer">“is_old”, to indicate whether each record belonged to the 2021 dataset (1) or the 2023 – 2024 dataset (0), derived from the sampling date column</paragraph>
 <paragraph index="51" node_type="writer">“is_twin”, to identify twin-hole relationships, where holes such as “F07-193” and “F07-193i” were flagged as twins (1) and non-twins (0)</paragraph>
 <paragraph index="52" node_type="writer">A consistency check was also carried out to ensure that the number of increments in the collar dataset matched the number of assay samples. Any disrepancies were manually reviewed and corrected when identified as typographical or sample alignment errors.</paragraph>
 <paragraph index="53" node_type="writer">After cleaning and verification, the processed datasets were imported into a PostgreSQL database using Dbeaver and Python scripts, establishing a structured data environment for analysis. A geometry column (geom) was generated from the X and Y coordinates to facilitate spatial querying and visualization.</paragraph>
 <paragraph index="54" node_type="writer">Normalization or </paragraph>
 <object name="Line 1" alt="" object_type="shape" description=""/>
 <object name="Horizontal line 1" alt="" object_type="shape" description=""/>
 <object name="Line 2" alt="" object_type="shape" description=""/>
</indexing>
